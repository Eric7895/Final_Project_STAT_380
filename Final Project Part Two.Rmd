---
title: 'Final Project: Report 2'
author: "Eric Wu, Caden Summers, Jacob O'Leary, Yu-Hsin Liao, Matthew Brady"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
editor_options:
  markdown:
    wrap: sentence
---

## Introduction

### Background

The purpose of this project is to take a deep look into the use of single-layer neural network.
In the part one of the project, we have examine the lab on single-layer neural network on Hitter's data.
The author is trying to compare and contrast model performances of three different models based on the mean absolute error of each model.
The result is significant, despite the fact that LASSO outperform the multiple linear regression and single-layer neural network, we still get to see how single-layer neural network produces a loss function graph and its basic structure and parameters.
For the second part of the project, our goal was to fit a similar single-layer neural network to another data set.

### Dataset

We decided to use the "House Rent Prediction" (HRP) dataset found on Kaggle, a data science community known for its enriched various data sources.
The HRP dataset consists of 4700+ observations and 11 attributes that allow us to make a prediction on the number of rents given a variety of attributes.

Here's the list of variables used and explained by the contributor of the dataset:

-   BHK: Number of Bedrooms, Hall, Kitchen

-   Rent: Rent of the Houses/Apartments/Flats

-   Size: Size of the Houses/Apartments/Flats in Square Feet

-   Floor: Houses/Apartments/Flats situated on which Floor and Total Number of Floors (Example: Ground out of 2, 3 out of 5, etc.)

-   Area Type: Size of the Houses/Apartments/Flats calculated on either Super Area or Carpet Area or Build Area.

-   Area Locality (Area.Locality): Locality of the Houses/Apartments/Flats

-   City: City where the Houses/Apartments/Flats are Located

-   Furnishing Status: Furnishing Status of the Houses/Apartments/Flats, either it is Furnished or Semi-Furnished or Unfurnished

-   Tenant Preferred: Type of Tenant Preferred by the Owner or Agent

-   Bathroom: Number of Bathrooms

-   Point of Contact (Posted.On): Whom should you contact for more information regarding the Houses/Apartments/Flats

```{r libraries and dataset}
#| echo = FALSE,
#| warning = FALSE,
#| message = FALSE

# Load essential libraries
library(tidyverse)
library(ggplot2)
library(keras)
library(ISLR2)
library(FNN)

# Load the dataset
hrp <- read.csv("House_Rent_Dataset.csv")

```

```{r data cleaning}
#| echo = FALSE

# Create indicator variables
hrp_update <- hrp %>%
  mutate(
    Area.indicator = case_when(
      Area.Type == "Built Area" ~ 1,
      Area.Type == "Carpet Area" ~ 2,
      Area.Type == "Super Area" ~ 3
    ),
    Furnishing.indicator = case_when(
      Furnishing.Status == "Furnished" ~ 1,
      Furnishing.Status == "Semi-Furnished" ~ 2,
      Furnishing.Status == "Unfurnished" ~ 3
    ),
    Contact.indicator = case_when(
      Point.of.Contact == "Contact Agent" ~ 1,
      Point.of.Contact == "Contact Builder" ~ 2,
      Point.of.Contact == "Contact Owner" ~ 3
    )
  )

```

```{r parameters and set ups}
#| echo = FALSE

# Set up all parameters
seed <- 123
seed_alter <- 12345

# Scale the data
xvars <- c("BHK", "Size", "Area.indicator", "Furnishing.indicator", "Bathroom", "Contact.indicator")
hrp_update[ , xvars] <- scale(hrp_update[ , xvars], center = TRUE, scale = TRUE)

# Set up the necessary matrix for neural network
x <- scale(model.matrix(Rent ~ BHK + Size + Area.indicator + Furnishing.indicator + Bathroom + Contact.indicator - 1, data = hrp_update))
y <- hrp_update$Rent

# Set up the training/testing split
set.seed(seed)
train_ind <- sample(1:nrow(hrp_update), floor(0.8 * nrow(hrp_update)))
set.seed(NULL)

set.seed(seed_alter)
train_ind_alter <- sample(1:nrow(hrp_update), floor(0.8 * nrow(hrp_update)))
set.seed(NULL)

# Not scaled data
train <- hrp[train_ind, ]
test <- hrp[-train_ind, ]

# Not scaled data with alternative seed
train_alter_lin <- hrp[train_ind_alter, ]
test_alter_lin <- hrp[-train_ind_alter, ]

# Scaled data
train_scale <- hrp_update[train_ind, ]
test_scale <- hrp_update[-train_ind, ]

# Scaled data with alternative seed
train_alter <- hrp_update[train_ind_alter, ]
test_alter <- hrp_update[-train_ind_alter, ]

```

## Exploratory Data Analysis

### Data Visualization

```{r}
hrp %>%
  group_by(Tenant.Preferred) %>%
  count()

```

```{r}
hrp %>%
  ggplot(mapping=aes(x=Size, y=Tenant.Preferred)) +
  geom_boxplot()

```

It seems as though houses that prefer families as tenants may be slightly larger on average than those that prefer bachelors, although the difference is not significant.
Houses with no preference, however, seem to be slightly smaller than both on average but there are many more upper outliers.
These results may be skewed however, as there are significantly more houses with no preference than those with a preference either way.

```{r}
hrp %>%
  ggplot(mapping=aes(x=Bathroom, y=Furnishing.Status)) +
  geom_boxplot()

```

Clearly Unfurnished houses tend to have less bathrooms than Semi-Furnished and Furnished houses, while Semi-Furnished and Furnished houses seem to generally have the same amount of bathrooms as each other.

### Hierarchical clustering

## Methodology

Our plan is to compare and contrast a multiple regression model, a KNN regression model, and a single-layer neural network in predicting the rent based using other predictors in the dataset.
All models will involve some sort of variable selection, whether it's using other functions or included in model building, this is very beneficial when we are doing model selections.
The primary focus is to compare the RMSE produced by each model.

### Multiple Linear Regression

```{r}
base_model <- lm(Rent ~ . - Area.Locality - Floor - Posted.On, data=train)
base_model2 <- lm(Rent ~ . - Area.Locality - Floor - Posted.On, data=train_alter_lin)

linear_model <- step(base_model, direction="backward")
linear_model2 <- step(base_model2, direction="backward")
```

```{r}
linpred <- predict(linear_model, newdata=test)

test_linpred <- 
  test %>%
  cbind(linpred) %>%
  mutate(Resid.Square = (Rent-linpred)^2)

sqrt(sum(test_linpred$Resid.Square)/nrow(test_linpred))
```

```{r}
linpred2 <- predict(linear_model2, newdata=test_alter_lin)

test_linpred2 <- 
  test_alter_lin %>%
  cbind(linpred2) %>%
  mutate(Resid.Square = (Rent-linpred2)^2)

sqrt(sum(test_linpred2$Resid.Square)/nrow(test_linpred2))
```

RMSE with seed 123 for multiple linear regression is 42803.14, while with seed 12345 RMSE is 39529.23.
So, there is a potential for slight variations in the RMSE based on the random seed, but not too significant, as this is a less than 10% difference in RMSE.

### KNN Regression

```{r}
xvars <- names(select_if(train_scale, is.numeric))
maxK <- 20 
mse_vec <- rep(NA, maxK)
mse_vec2 <- rep(NA, maxK)
rmse_vec <- rep(NA, maxK)
rmse_vec2 <- rep(NA, maxK)

for(i in 1:maxK){
  knn_res <- knn.reg(train = train_scale[ , xvars, drop = FALSE],
                   test = test_scale[ , xvars, drop = FALSE],
                   y = train_scale$Rent,
                   k = i)
  knn_res2 <- knn.reg(train = train_alter[ , xvars, drop = FALSE],
                   test = test_alter[ , xvars, drop = FALSE],
                   y = train_alter$Rent,
                   k = i)
  
  mse_vec[i] <- mean((test_scale$Rent - knn_res$pred)^2)
  mse_vec2[i] <- mean((test_alter$Rent - knn_res2$pred)^2)

  rmse_vec[i] <- sqrt(mse_vec[i])
  rmse_vec2[i] <- sqrt(mse_vec2[i])
}
choice_k <- data.frame(k = 1:maxK, rmse = rmse_vec)
ggplot(data = choice_k, mapping = aes(x = k, y = rmse)) +
   geom_line()

choice_k2 <- data.frame(k = 1:maxK, rmse = rmse_vec2)
ggplot(data = choice_k2, mapping = aes(x = k, y = rmse)) +
   geom_line()
# Based on the graph the best K value seems to be the lowest point on the graph.

```

```{r}
# Based on the front of this vector the best K value is 2.
head(choice_k)
head(choice_k2)
# The KNN regression is also not immune to seed changes and in this case results in an RMSE that is 1600 lower than the first seed tested.
# Although the RMSE has changed significantly, it is still far better than the other methods presented
```

### Single-Layer Neural Network

```{r single layer neural network}
modnn <- keras_model_sequential() %>%
     layer_dense(units = 2351, activation = "relu", 
        input_shape = ncol(x)) %>% 
     layer_dropout(rate = 0.4) %>% 
     layer_dense(units = 1)

modnn %>% compile(loss = "mse",  
    optimizer = optimizer_rmsprop(),
    metrics = list("mse")
   )

#history <- modnn %>% fit(
#     x[train_ind, ], y[train_ind], epochs = 600, batch_size = 32, # 
#    validation_data = list(x[-train_ind, ], y[-train_ind]) 
#  )

#npred <- predict(modnn, x[-train_ind, ])
#nn_mse <- mean((y[-train_ind] - npred)^2)
#sqrt(nn_mse)

## 43418.85

```

![Image 1: Training Results from single-layer Neural Network](C:/Users/wue77/Documents/GitHub/Final_Project_STAT_380/Rplot.PNG)

The structure of the single-layer neural network is very similar to the lab, except for the 2351 hidden units, and we are measuring the Mean Square Error(MSE) instead of the Mean Absolute Error(MAE).
The RMSE resulting from the training end up being 43418.85.
And base on the loss function graph, the MSE of the validation(testing) set is significantly smaller than the training set.
Which indicates better performance in the testing set.

```{r impact of seed on neural network}
#modnn2 <- keras_model_sequential() %>%
#     layer_dense(units = 2351, activation = "relu", 
#        input_shape = ncol(x)) %>% 
#     layer_dropout(rate = 0.4) %>% 
#     layer_dense(units = 1)

#modnn2 %>% compile(loss = "mse",  
#    optimizer = optimizer_rmsprop(),
#    metrics = list("mse")
#   )

#history2 <- modnn2 %>% fit(
#     x[train_ind_alter, ], y[train_ind_alter], epochs = 600, batch_size = 32, # 
#    validation_data = list(x[-train_ind_alter, ], y[-train_ind_alter]) 
#  )

#npred2 <- predict(modnn2, x[-train_ind_alter, ])
#nn_mse2 <- mean((y[-train_ind_alter] - npred2)^2)
#sqrt(nn_mse2)

# 37620.59

```

On the other hand, the single-layer neural network are not resistant to changed of seed, as the MSE is greatly reduced and the RMSE decreases from 43418.85 to 37620.59. Unfortunately, the picture of the loss function graph is distorted, and we wouldn't able to placed the page here but rather at the end. 

## Discussion
After testing our three methods on the House Rent dataset with the seed set to 123, we got RMSE values of 42,803.14, 2014.33, and 43,418.85 for Multiple Linear regression, KNN regression, and Single-Layer Neural Network respectively. Based on these values it is very clear that the KNN method with a k-value of 2 is significantly better than both of these methods. The performance is roughly 20 times better with this metric alone. We also wanted to test the models accuracy against varying seed values to ensure that they are robust and not easily affected by small changes. From this testing we confirmed that none of the models were immune to changes in the seed. The KNN regression in particular was able to get its RMSE as low as 400 which is far lower than the neural networks ~37,620. 

## Conclusion

Overall, the project is very successful and we get to review and learn more about the implementation of various methods that was taught in class. In addition, the project allows us to examine the parameters of single-layer neural network, and the optimization function that goes along with the train process. 

## Code Appendix
```{r code Appendix}
#| ref.label = knitr::all_labels(),
#| echo = TRUE,
#| eval = FALSE

```

![Image 2: Training Results from single-layer Neural Network using alternative seed](C:/Users/wue77/Documents/GitHub/Final_Project_STAT_380/Rplot2.PNG)

## Contributions
Caden: KNN regression, Discussion
Jacob: Multiple Linear Regression, Data Visualization
